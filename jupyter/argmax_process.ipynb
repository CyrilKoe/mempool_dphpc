{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "sns.reset_orig()\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "remote_paths=[]\n",
    "remote_paths.append(\"\")\n",
    "remote_paths.append(\"/usr/scratch2/larain8/cykoenig/development/mempool_dphpc/jupyter/\")\n",
    "\n",
    "#path to your input result folder\n",
    "\n",
    "#path to the output csv\n",
    "csv_path= \"./argmax_results_3.csv\"\n",
    "\n",
    "\n",
    "def is_notebook() -> bool:\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            return True   # Jupyter notebook or qtconsole\n",
    "        elif shell == 'TerminalInteractiveShell':\n",
    "            return False  # Terminal running IPython\n",
    "        else:\n",
    "            return False  # Other type (?)\n",
    "    except NameError:\n",
    "        return False      # Probably standard Python interpreter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_keep = ['num_cores', 'num_data', 'core', 'section', 'seed', 'max_val', 'cs_retry', 'cs_duration', 'cycles', 'algo']\n",
    "\n",
    "tot_df = pd.DataFrame()\n",
    "\n",
    "for remote_path in remote_paths:\n",
    "    \n",
    "    result_path = remote_path+\"../hardware/results\"\n",
    "    print(\"\\nProcessing files at\", result_path)\n",
    "\n",
    "    # Add the lock informations in the results.csv files\n",
    "    !python ../onnx_benchmark/analyze_locks.py VERBOSE=0 REMOTE_PATH={remote_path}\n",
    "\n",
    "    todo, done = 0, 0\n",
    "    for (dirpath, dirnames, filenames) in walk(result_path):\n",
    "        for filename in filenames:\n",
    "            if filename == \"transcript\":\n",
    "                todo+=1\n",
    "\n",
    "    configs = []\n",
    "    for (dirpath, dirnames, filenames) in walk(result_path):\n",
    "        num_cores = -1\n",
    "        num_data = -1\n",
    "        num_local_data = -1\n",
    "        max_val = -1\n",
    "        seed = -1\n",
    "        to_find = {\"lobal max\":[], \"ndexes len\":[]}\n",
    "        stop = False\n",
    "            \n",
    "        for filename in filenames:\n",
    "            if filename != \"transcript\":\n",
    "                continue\n",
    "            p = re.compile(\"[0-9]+\")\n",
    "            # Get parameters from the directory name first\n",
    "            num_cores, num_data, max_val, seed = dirpath.split('/')[-2].split('_')[-4:]\n",
    "            algo = \"_\".join(dirpath.split('/')[-2].split('_')[:-4])\n",
    "            num_cores, num_data, max_val, seed = int(num_cores), int(num_data), int(max_val), int(seed)\n",
    "            nul_local_data = int(num_data / num_cores)\n",
    "        \n",
    "            # Override them based on transcript if any\n",
    "            with open(dirpath+\"/\"+filename, \"r\") as fp:\n",
    "\n",
    "                for line in fp.readlines():\n",
    "                    found = False\n",
    "                    for word in to_find.keys():\n",
    "                        if word in line and not found:\n",
    "                            to_find[word].append([a.group() for a in re.compile(\"[0-9]+\").finditer(line)][0])\n",
    "                            found = True\n",
    "                \n",
    "                    if \"ERROR\" in line:\n",
    "                        print(\"ERROR in \", dirpath)\n",
    "                        assert(0)\n",
    "                assert(to_find[\"lobal max\"][0]==to_find[\"lobal max\"][1] and to_find[\"ndexes len\"][0]==to_find[\"ndexes len\"][1])\n",
    "\n",
    "            # Read CSV\n",
    "            done += 1\n",
    "            df = pd.read_csv(dirpath+\"/results.csv\")\n",
    "            df[\"num_cores\"] = int(num_cores)\n",
    "            df[\"num_data\"] = int(num_data)\n",
    "            df[\"num_local_data\"] = int(num_local_data)\n",
    "            df[\"max_val\"] = int(max_val)\n",
    "            df[\"seed\"] = int(seed)\n",
    "            df[\"algo\"] = algo\n",
    "            \n",
    "            # Delete column without name\n",
    "            df = df.reset_index(drop=True)\n",
    "            for col in df.columns:\n",
    "                if not col in cols_to_keep:\n",
    "                    df = df.drop(col, 1)\n",
    "            # Print state\n",
    "            print(\"\\r{}/{}\".format(done, todo) + \"    cores={0:3d}\".format(int(num_cores))+\"    data={0:7d}b\".format(int(num_data))\\\n",
    "                  +\"    max_gen={0:7d}\".format(int(max_val))+\"    max={0:7d}\".format(int(to_find['lobal max'][0]))+\"    len={0:7d}\".format(int(to_find['ndexes len'][0])), end=\"\")\n",
    "            # Concat everyone\n",
    "            configs.append([num_cores, num_data, num_local_data])\n",
    "            tot_df = pd.concat([tot_df, df], axis=0, join=\"outer\", ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)\n",
    "            if(tot_df['cs_retry'].isnull().any()):\n",
    "                print(\"\\r\"+dirpath+\"/\"+filename+\" uncorrect cs_retry, rerun analyze_locks\")\n",
    "            if(tot_df['core'].isnull().any()):\n",
    "                print(\"\\r\"+dirpath+\"/\"+filename+\" uncorrect core\")\n",
    "        \n",
    "#raw_df = tot_df.copy()\n",
    "# Drop nans\n",
    "to_drop = tot_df.columns[tot_df.isnull().any()]\n",
    "print(\"\\nDropping :\")\n",
    "for nan_col in to_drop:\n",
    "    assert(not nan_col in ['cycles', 'num_data', 'num_cores'])\n",
    "    tot_df = tot_df.drop(nan_col, axis=1)\n",
    "    print(nan_col, end=\" \")\n",
    "\n",
    "# Drop strings\n",
    "tot_df = tot_df #.select_dtypes(exclude=['object'])\n",
    "\n",
    "tot_df.to_csv(csv_path)\n",
    "print(\"\\nResults exported to\", csv_path)\n",
    "\n",
    "print(tot_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tot_df = pd.read_csv(csv_path)\n",
    "print(tot_df.shape)\n",
    "print(tot_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iddx of the CI of the median with 95%\n",
    "def ci_idx(n):\n",
    "    return [int((n-1.96*np.sqrt(n))/2), int(np.ceil(1+(n+1.96*np.sqrt(n))/2))]\n",
    "\n",
    "# return (ci_low, median, ci_high) for a list of values\n",
    "def median_and_ci(data):\n",
    "    n = len(data)\n",
    "    a, b = ci_idx(n)\n",
    "    data_sorted = sorted(data)\n",
    "    return data_sorted[a], data_sorted[int(n/2)], data_sorted[min(b, n-1)]\n",
    "\n",
    "def get_median(data):\n",
    "    return median_and_ci(data)[1]\n",
    "def get_ci(data):\n",
    "    a, b, c = median_and_ci(data)\n",
    "    return a, c\n",
    "\n",
    "def get_ideal_speedup(a, b, n=100):\n",
    "    X = np.linspace(a, b, num=n)\n",
    "    return X, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tot_df :\", tot_df.shape)\n",
    "\n",
    "colors = [(4, \"green\"), (64, \"green\"), (200, \"green\")]\n",
    "\n",
    "\n",
    "#\n",
    "# Strong scaling with confidence intervals\n",
    "#\n",
    "\n",
    "print(tot_df['num_data'].unique())\n",
    "print(tot_df['num_cores'].unique())\n",
    "\n",
    "\n",
    "algo = \"argmax_impl\"\n",
    "for num_data, color in colors: #tot_df['num_data'].unique():\n",
    "    experiment_df = tot_df[(tot_df['num_data'] == num_data) & ((tot_df['algo'] == \"argmax_base\")|(tot_df['algo'] == algo))]\n",
    "    print(\"Experiment :\", experiment_df.shape)\n",
    "    Y1, Y1_low, Y1_high = [], [], []\n",
    "    Y1_fill, Y1_argmax, Y1_reduction = [], [], []\n",
    "    num_cores_arr = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    all_cores = []\n",
    "    all_tot = []\n",
    "    for num_cores in num_cores_arr:\n",
    "        res_fill = []\n",
    "        res_argmax = []\n",
    "        res_reduction = []\n",
    "        res_tot = []\n",
    "        maxs = []\n",
    "        for seed in experiment_df['seed'].unique():\n",
    "            mask = (experiment_df['seed'] == seed) & (experiment_df['num_cores'] == num_cores)\n",
    "            masked = experiment_df[mask]\n",
    "            if masked.empty:\n",
    "                continue\n",
    "            res_fill.append(      masked.loc[masked['section'] == 0]['cycles'].max() / 1000000 )\n",
    "            res_argmax.append(    masked.loc[masked['section'] == 1]['cycles'].max() / 1000000 )\n",
    "            res_reduction.append( masked.loc[masked['section'] == 2]['cycles'].max() / 1000000 )\n",
    "            # All does not count fill section\n",
    "            res_tot.append(       masked[masked['section']==1].groupby(['core']).cycles.sum().max() / 1000000)\n",
    "            all_cores.append(     num_cores)\n",
    "            all_tot.append(       masked[masked['section']!=0].groupby(['core']).cycles.sum().max() / 1000000)\n",
    "        if len(res_tot) == 0:\n",
    "            continue\n",
    "        lo, median, hi = median_and_ci(res_tot)\n",
    "        Y1.append(median)\n",
    "        Y1_low.append(lo)\n",
    "        Y1_high.append(hi)\n",
    "        Y1_fill.append(get_median(res_fill))\n",
    "        Y1_argmax.append(get_median(res_argmax))\n",
    "        Y1_reduction.append(get_median(res_reduction))\n",
    "        print(\"(\", num_cores, \",\", len(res_tot), \")\", end=\" : \")\n",
    "        print(\"(\", lo, \",\", median, \",\", hi, \")\")\n",
    "    \n",
    "    Y1 = np.array(Y1)\n",
    "    all_cores = np.array(all_cores)\n",
    "    all_tot = np.array(all_tot)\n",
    "    print(all_cores)\n",
    "    \n",
    "    #plot_errorbars()\n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.set_xscale('log', basex=2)\n",
    "    ax.set_xlim(1, 256)\n",
    "    ax.set_xticks(num_cores_arr)\n",
    "    ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "    #sns.pointplot(num_cores_arr, Y1[0]/Y1, errorbar=get_ci, join=False, ax=ax)\n",
    "    #sns.pointplot(num_cores_arr, 2*Y1[0]/Y1, errorbar=get_ci, join=False, ax=ax)\n",
    "    plt.plot(num_cores_arr, Y1[0]/Y1, '-o', label=\"Observed\")\n",
    "    plt.plot(get_ideal_speedup(1, 256)[0], get_ideal_speedup(1, 256)[1], '--', color=\"red\", label=\"Ideal\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Argmax strong scaling with \" + str(num_data * 256) + \" datas (int32)\")\n",
    "    plt.xlabel(\"# cores\")\n",
    "    plt.ylabel(\"Speedup\")\n",
    "    \n",
    "    print(Y1[0]/Y1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plot_df = pd.DataFrame({'Cores': num_cores_arr, 'Fill': Y1_fill, 'Argmax': Y1_argmax, 'Reduction': Y1_reduction}, \\\n",
    "                           columns = ['Cores', 'Fill', 'Argmax', 'Reduction'])\n",
    "    plot_df = plot_df.set_index('Cores')\n",
    "    plot_df.drop(columns=['Fill']).plot(kind='bar', stacked=True, color=['orange', 'blue'], ax=ax)\n",
    "    plt.title(\"Computing time distribution with \" + str(num_data * 256) + \" datas (int32)\")\n",
    "    plt.xlabel(\"# cores\")\n",
    "    plt.ylabel(\"Time (Mcycles)\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 6), dpi=80)\n",
    "    filter_box = all_cores == 1\n",
    "    \n",
    "    # Remove median\n",
    "    for num_cores in num_cores_arr:\n",
    "        idx = np.where(all_cores == num_cores)\n",
    "        median = get_median(all_tot[idx])\n",
    "        #all_tot[idx] -= median\n",
    "\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    plot_df = pd.DataFrame({'Cores': all_cores,\n",
    "                            'Total': all_tot}, \\\n",
    "                           columns = ['Cores', 'Total'])\n",
    "    sns.boxplot(data=plot_df, x=\"Cores\", y=\"Total\", whis=10, ax=ax)\n",
    "    plt.title(\"Variability with \" + str(num_data * 256) + \" datas (int32)\")\n",
    "    plt.xlabel(\"# cores\")\n",
    "    plt.ylabel(\"Time (Mcycles)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Tot_df :\", tot_df.shape)\n",
    "\n",
    "colors = [(1, \"Serial\"), (256, \"Parallel\")]\n",
    "algos = [('argmax_impl_2', 'lightgreen'), ('argmax_impl', 'lightblue')]\n",
    "\n",
    "legend_elements = []\n",
    "for a, b in algos:\n",
    "    legend_elements+=[Line2D([0], [0], color=b, lw=4, label=a)]\n",
    "\n",
    "#plot_errorbars()\n",
    "fig = plt.figure(0, figsize=(8, 6), dpi=80)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_xscale('log', basex=2)\n",
    "ax.set_xlim(1, 256)\n",
    "ax.set_xticks(sorted(tot_df['num_data'].unique()))\n",
    "ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "plt.title(\"Speedup with input size\")\n",
    "plt.xlabel(\"Input size (KiB)\")\n",
    "plt.ylabel(\"Time (Mcycles)\")\n",
    "\n",
    "#\n",
    "# Strong scaling with confidence intervals\n",
    "#\n",
    "\n",
    "\n",
    "Y1_0 = []\n",
    "for num_cores, label in colors: #tot_df['num_data'].unique():\n",
    "    for algo, algo_color in algos:\n",
    "        experiment_df = tot_df[(tot_df['num_cores'] == num_cores) & ((tot_df['algo'] == \"argmax_base\")|(tot_df['algo'] == algo))]\n",
    "        print(\"Num cores:\", num_cores,\"Experiment :\", experiment_df.shape)\n",
    "        Y1, Y1_low, Y1_high = [], [], []\n",
    "        Y1_fill, Y1_argmax, Y1_reduction = [], [], []\n",
    "        num_data_arr = sorted(tot_df['num_data'].unique())\n",
    "        all_data = []\n",
    "        all_tot = []\n",
    "    \n",
    "        mask = (experiment_df['num_data'] == 200)\n",
    "    \n",
    "        for num_data in num_data_arr:\n",
    "            res_fill = []\n",
    "            res_argmax = []\n",
    "            res_reduction = []\n",
    "            res_tot = []\n",
    "            for seed in experiment_df['seed'].unique():\n",
    "                mask = (experiment_df['seed'] == seed) & (experiment_df['num_data'] == num_data)\n",
    "                masked = experiment_df[mask]\n",
    "                if masked.empty:\n",
    "                    continue\n",
    "                res_fill.append(      masked.loc[masked['section'] == 0]['cycles'].max() / 1000000 )\n",
    "                res_argmax.append(    masked.loc[masked['section'] == 1]['cycles'].max() / 1000000 )\n",
    "                res_reduction.append( masked.loc[masked['section'] == 2]['cycles'].max() / 1000000 )\n",
    "                # All does not count fill section\n",
    "                res_tot.append(       masked[masked['section']!=0].groupby(['core']).cycles.sum().max() / 1000000)\n",
    "                all_data.append(      num_data)\n",
    "                all_tot.append(       masked[masked['section']!=0].groupby(['core']).cycles.sum().max() / 1000000)\n",
    "            if len(res_tot) == 0:\n",
    "                continue\n",
    "            lo, median, hi = median_and_ci(res_tot)\n",
    "            Y1.append(median)\n",
    "            Y1_low.append(lo)\n",
    "            Y1_high.append(hi)\n",
    "            Y1_fill.append(get_median(res_fill))\n",
    "            Y1_argmax.append(get_median(res_argmax))\n",
    "            Y1_reduction.append(get_median(res_reduction))\n",
    "            print(\"(\", num_data, \",\", len(res_tot), \")\", end=\" : \")\n",
    "            print(\"(\", lo, \",\", median, \",\", hi, \")\")\n",
    "    \n",
    "        Y1 = np.array(Y1)\n",
    "        all_tot = np.array(all_tot)\n",
    "        fig = plt.figure(0, figsize=(8, 6), dpi=80)\n",
    "        plt.plot(num_data_arr, Y1, \"-o\", label=label)\n",
    "        \n",
    "        if num_cores != 1:\n",
    "            #plt.plot(num_data_arr, Y1_0/Y1, \"-o\", label=label)\n",
    "            pass\n",
    "    \n",
    "        if num_cores == 1:\n",
    "            Y1_0 = Y1\n",
    "    \n",
    "        new_fig = plt.figure(num_cores, figsize=(8, 6), dpi=80)\n",
    "        # Remove median\n",
    "        for num_data in num_data_arr:\n",
    "            idx = np.where(all_data == num_data)\n",
    "            #median = get_median(all_tot[idx])\n",
    "            #all_tot[idx] -= median\n",
    "\n",
    "        ax2 = new_fig.add_subplot(1, 1, 1)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        plot_df = pd.DataFrame({'Datas': all_data,\n",
    "                                'Total': all_tot}, \\\n",
    "                               columns = ['Datas', 'Total'])\n",
    "        sns.boxplot(data=plot_df, x=\"Datas\", y=\"Total\", whis=10, ax=ax2, color=algo_color)\n",
    "        plt.title(\"Execution time with input size for \"+str(num_cores)+\" cores\")\n",
    "        plt.xlabel(\"Input size (KiB)\")\n",
    "        plt.ylabel(\"Time (Mcycles)\")\n",
    "        plt.legend(handles=legend_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = tot_df.groupby([\"num_cores\", \"num_data\", \"section\"]).agg({'cs_retry': 'sum', 'cs_duration': 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_cores = 256\n",
    "num_data = 200\n",
    "section = 1\n",
    "\n",
    "for num_data in [8, 32, 64, 200]:\n",
    "    plt.figure()\n",
    "    strin = grouped.loc[num_cores,num_data,section]['cs_retry'].replace('][',',').replace('[', '').replace(']', '').replace(' ','')\n",
    "    distrib = [int(x) for x in strin.split(',') if x != '']\n",
    "    plt.hist(distrib, bins=20, density=True)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
